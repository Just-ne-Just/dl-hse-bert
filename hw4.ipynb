{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2211cadf",
   "metadata": {
    "cellId": "0i0qd5fh2kc1t117d2oct5"
   },
   "source": [
    "# Глубинное обучение для текстовых данных, ФКН ВШЭ\n",
    "## Домашнее задание 4: уменьшение размеров модели\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Максимально допустимая оценка за работу — __14 баллов__. Сдавать задание после указанного срока сдачи нельзя.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Весь код должен быть написан самостоятельно. Чужим кодом для пользоваться запрещается даже с указанием ссылки на источник. В разумных рамках, конечно. Взять пару очевидных строчек кода для реализации какого-то небольшого функционала можно.\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке. Также оценка может быть снижена за плохо читаемый код и плохо оформленные графики. Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n",
    "\n",
    "__Мягкий дедлайн 20.12.23__ \\\n",
    "__Жесткий дедлайн 20.12.23__\n",
    "\n",
    "### О задании\n",
    "\n",
    "В этом задании вам предстоит научиться решать задачу Named Entity Recognition (NER) на самом популярном датасете – [CoNLL-2003](https://paperswithcode.com/dataset/conll-2003). В вашем распоряжении будет предобученный BERT, который вам необходимо уменьшить без потерь в качестве. Задание разделено на две части. Первая часть состоит из набора методов по уменьшению модели, которые нужно реализовать по инструкции. Вторая часть – это творческое соревнование, в котором вы можете пользоваться любыми методами, кроме ансамблирования и использования дополнительных данных. Дополнительное условие соревнования: размер вашей модели __не может превышать 20M параметров__.\n",
    "\n",
    "__!!ВАЖНО!!__ Вам придется проводить довольно много экспериментов, поэтому мы рекомендуем не писать весь код в тетрадке, а завести разные файлы для отдельных логических блоков и скомпоновать все в виде проекта. Это позволит вашему ноутбуку не разрастаться и сильно облегчит задачу и вам, и проверяющим.\n",
    "\n",
    "\n",
    "### О датасете\n",
    "\n",
    "В CoNLL-2003 для именования сущностей используется маркировка **BIO** (Beggining, Inside, Outside), в которой метки означают следующее:\n",
    "\n",
    "- *B-{метка}* – начало сущности *{метка}*\n",
    "- *I-{метка}* – продолжнение сущности *{метка}*\n",
    "- *O* – не сущность\n",
    "\n",
    "Существуют так же и другие способы маркировки, например, BILUO. Почитать о них можно [тут](https://en.wikipedia.org/wiki/Inside–outside–beginning_(tagging)) и [тут](https://www.youtube.com/watch?v=dQw4w9WgXcQ).\n",
    "\n",
    "Всего в датасете есть 9 разных меток.\n",
    "- O – слову не соответствует ни одна сущность.\n",
    "- B-PER/I-PER – слово или набор слов соответстует определенному _человеку_.\n",
    "- B-ORG/I-ORG – слово или набор слов соответстует определенной _организации_.\n",
    "- B-LOC/I-LOC – слово или набор слов соответстует определенной _локации_.\n",
    "- B-MISC/I-MISC – слово или набор слов соответстует сущности, которая не относится ни к одной из предыдущих. Например, национальность, произведение искусства, мероприятие и т.д.\n",
    "\n",
    "Приступим!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e1b013ec",
   "metadata": {
    "cellId": "b3rbvr4tauoecaicb0ur8k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/12/dd/f17b11a93a9ca27728e12512d167eb1281c151c4c6881d3ab59eb58f4127/transformers-4.35.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/05/09/1945ca6ba3ad8ad6e2872ba682ce8d68c5e63c8e55458ed8ab4885709f1d/huggingface_hub-0.19.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /kernel/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/eb/3d/eee5f3c572a3f4db2ebabf5bd4f284f356078a5b5d27e6229b4450d5c5e4/tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/35/8f/892d2e1bcfceb7ee3f9b055ac4bb111e31d25f0a38c7f44d1d59bf7a501a/safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /kernel/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "Collecting charset-normalizer~=2.0.0 (from requests->transformers)\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /kernel/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, charset-normalizer, huggingface-hub, tokenizers, transformers\n",
      "\u001b[33m  WARNING: The script normalizer is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script transformers-cli is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed charset-normalizer-2.0.12 huggingface-hub-0.19.4 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.35.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "1146d563",
   "metadata": {
    "cellId": "6mqhujxcqgnvc4shts4l"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "951b3bb5",
   "metadata": {
    "cellId": "kfqrs3yvz9a7n8riutnyj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 09:23:40.121104: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, DataCollatorForTokenClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Dict, List\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99b5a3c",
   "metadata": {
    "cellId": "qd8uezcyq3g3j5h1lrl8rs",
    "execution_id": "471e16b3-12d8-43d9-b1b5-9b388238b3f3"
   },
   "source": [
    "__Задание 1 (0.5 балла)__ Допишите функцию `read_conll2003` для чтения датасета. Внутри она должна проитерироваться по всем строкам файла и для каждого примера составить словарь с полями `words` и `tags` (слова и тэги текста соответственно). На выход функция возвращает список полученных словарей. Тексты в файле разделяются переносом строки `\\n`, а слова и тэги – проблелом. Пример:\n",
    "```\n",
    "! head -n 15 CoNLL2003/train.txt\n",
    "\n",
    "EU B-ORG\n",
    "rejects O\n",
    "German B-MISC\n",
    "call O\n",
    "to O\n",
    "boycott O\n",
    "British B-MISC\n",
    "lamb O\n",
    ". O\n",
    "\n",
    "Peter B-PER\n",
    "Blackburn I-PER\n",
    "\n",
    "BRUSSELS B-LOC\n",
    "1996-08-22 O\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "d8ec38f0",
   "metadata": {
    "cellId": "q5txx64h7djg5o80f7poc9"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def read_conll2003(path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Read data in CoNNL like format.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    dataset.append({\n",
    "        \"words\": [],\n",
    "        \"tags\": []\n",
    "    })\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        all_texts = f.readlines()\n",
    "        all_texts = list(map(lambda x: x.strip(), all_texts))\n",
    "    \n",
    "    for text in all_texts:\n",
    "        if text == '':\n",
    "            if dataset[-1][\"words\"]:\n",
    "                dataset.append({\n",
    "                    \"words\": [],\n",
    "                    \"tags\": []\n",
    "                })\n",
    "        else:\n",
    "            word, tag = text.split()\n",
    "            dataset[-1][\"words\"].append(word)\n",
    "            dataset[-1][\"tags\"].append(tag)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e680ce",
   "metadata": {
    "cellId": "cvr4sp413ovjlhah8joir",
    "execution_id": "2e2924b6-2087-4e96-81af-0e55d98a4723"
   },
   "source": [
    "Прочитаем тренировочный и валидационный датасеты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "3ee85a2b",
   "metadata": {
    "cellId": "aqx0yma954di3saromlo9g"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_dataset = read_conll2003(\"CoNLL2003/train.txt\")\n",
    "valid_dataset = read_conll2003(\"CoNLL2003/valid.txt\")\n",
    "\n",
    "tags = ['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n",
    "\n",
    "tag2id = {}\n",
    "for i, tag in enumerate(tags):\n",
    "    tag2id[tag] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "b0b156c6",
   "metadata": {
    "cellId": "rr63pzniuwa4ocw67l8ans"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU\tB-ORG\n",
      "rejects\tO\n",
      "German\tB-MISC\n",
      "call\tO\n",
      "to\tO\n",
      "boycott\tO\n",
      "British\tB-MISC\n",
      "lamb\tO\n",
      ".\tO\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "sample = train_dataset[0]\n",
    "\n",
    "assert sample['words'] == ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
    "assert sample['tags'] == ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n",
    "\n",
    "for w, t in zip(sample['words'], sample['tags']):\n",
    "    print(f'{w}\\t{t}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b339c5",
   "metadata": {
    "cellId": "ki0ta3pasbxn22b7wtw67",
    "execution_id": "011c348f-d101-4778-8fc5-9ceb2a3591b1"
   },
   "source": [
    "На протяжении всего домашнего задания мы будем использовать _cased_ версию BERT, то есть токенизатор будет учитывать регистр слов. Для задачи NER регистр важен, так как имена и названия организаций или предметов искусства часто пишутся с большой буквы, и будет глупо прятать от модели такую информацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "9c1f8906",
   "metadata": {
    "cellId": "go8bdq3h3dsd8r98tx9oi"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bcba5d95d934131850977474d25e688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c0e6db8e9247b8a315c0ade8b87821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd561c4685854c24ad36073bab610c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f6f2d3fa6a44b58c8c2d89719bb9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78819864",
   "metadata": {
    "cellId": "sgas83mftge3jt1q97p5sd",
    "execution_id": "66484a6d-6cf7-4650-aa02-66b4b83ecc25"
   },
   "source": [
    "Заметьте, что при токенизации слова могут разделиться на несколько токенов (как слово `lamb` из примера ниже), из-за чего появится несоответствие между числом токенов и тэгов. Это несоответствие нам придется устранить вручную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "2a7e39fc",
   "metadata": {
    "cellId": "km5k2v8hzio67x4n3fb5zo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слова:  ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "Токены: ['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "inputs = tokenizer(sample['words'], is_split_into_words=True)\n",
    "print('Слова: ', sample['words'])\n",
    "print('Токены:', inputs.tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c2400d",
   "metadata": {
    "cellId": "lfdkz30sybjwbeonemo9x",
    "execution_id": "0f71ef12-976d-4f29-a082-f642724fd173"
   },
   "source": [
    "К счастью, из выхода токенизатора можно достать список с номерами слов, к которым относится каждый токен. Если номер встретился несколько раз подряд, то слово разделилось. Специальные символы не принадлежат никакому слову, поэтому их номер – `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "d1a1244d",
   "metadata": {
    "cellId": "nokksmoswfoj03h4djulq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8432595",
   "metadata": {
    "cellId": "d3az0bjj7jmgdfdbgiuacb",
    "execution_id": "13bef543-2e21-474d-a75b-a636291944ff"
   },
   "source": [
    "__Задание 2 (0.5 балла)__ Допишите метод `get_inputs_and_aligned_labels` класса `Dataset`. Он принимает в себя объект из прочитанного выше датасета, токенизирует слова и выравнивает тэги. Выравнивание происходит следующим образом: если токен пренадлежит тому же слову, что и предыдущий токен, и его тэг начинается на `B`, то надо поменять `B` на `I`, потому что это уже продолжение сущности; в любом другом случае тэг токена остается таким же, какой был у соответствующего ему слова.\n",
    "\n",
    "Метод позвращает словарь с полями `input_ids` – результат токенизации, `labels` – индексы тэгов для каждого токена из маппинга `tag2id`, для специальных символов в качестве лейбла укажите -100, так как это значение по умолчанию, которое игнорируется при подсчете кросс-энтропии в классе `CrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "1a0f48a5",
   "metadata": {
    "cellId": "0k5avshy7zv9muzurgtw55c"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class Dataset:\n",
    "    def __init__(self, raw_dataset: List[Dict[str, str]], tag2id: Dict[str, int]):\n",
    "        \"\"\"\n",
    "        :params:\n",
    "        raw_dataset: output of read_conll2003 function\n",
    "        tag2id: mapping from tag name to its id\n",
    "        \"\"\"\n",
    "        self.dataset = raw_dataset\n",
    "        self.tag2id = tag2id\n",
    "\n",
    "    def get_inputs_and_aligned_labels(self, sample):\n",
    "        \"\"\"\n",
    "        Aligns tags with tokens and returns dict with token ids and tag ids.\n",
    "        \"\"\"\n",
    "        tokenized = tokenizer(sample['words'], is_split_into_words=True)\n",
    "        tags = sample['tags']\n",
    "\n",
    "        labels = []\n",
    "        word_ids = tokenized.word_ids()\n",
    "        for i, idx in enumerate(word_ids):\n",
    "            if idx is None:\n",
    "                labels.append(-100)\n",
    "            elif idx == word_ids[i - 1] and tags[idx][0] == 'B':\n",
    "                labels.append(tag2id['I' + tags[idx][1:]])\n",
    "            else:\n",
    "                labels.append(tag2id[tags[idx]])\n",
    "\n",
    "        return {\n",
    "            'input_ids': tokenized['input_ids'],\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        return self.get_inputs_and_aligned_labels(sample)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "3be11ebd",
   "metadata": {
    "cellId": "l3ym3mey7qrppgxacwlaf"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "tag2id = {tag: i for i, tag in enumerate(tags)}\n",
    "id2tag = {i: tag for tag, i in tag2id.items()}\n",
    "\n",
    "train_dataset = Dataset(train_dataset, tag2id)\n",
    "valid_dataset = Dataset(valid_dataset, tag2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "d622dc4b",
   "metadata": {
    "cellId": "vpq8nckdvtlp8o4hw4ii"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\t[CLS]\t-100\t\n",
      "7270\tEU\t2\tB-ORG\n",
      "22961\trejects\t8\tO\n",
      "1528\tGerman\t1\tB-MISC\n",
      "1840\tcall\t8\tO\n",
      "1106\tto\t8\tO\n",
      "21423\tboycott\t8\tO\n",
      "1418\tBritish\t1\tB-MISC\n",
      "2495\tla\t8\tO\n",
      "12913\t##mb\t8\tO\n",
      "119\t.\t8\tO\n",
      "102\t[SEP]\t-100\t\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "sample = train_dataset[0]\n",
    "\n",
    "input_ids, labels = sample['input_ids'], sample['labels']\n",
    "\n",
    "assert input_ids == [101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102]\n",
    "assert labels == [-100, 2, 8, 1, 8, 8, 8, 1, 8, 8, 8, -100]\n",
    "\n",
    "for idx, token, label in zip(input_ids, tokenizer.convert_ids_to_tokens(input_ids), labels):\n",
    "    tag = id2tag[label] if label != -100 else ''\n",
    "    print(f'{idx}\\t{token}\\t{label}\\t{tag}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2b9efb",
   "metadata": {
    "cellId": "clr45aucimgycazfwawdcd",
    "execution_id": "3d3d7869-e8a3-4870-ae0a-fd8fa9aacf16"
   },
   "source": [
    "На данный момент наш датасет возвращает по индексу списки токенов и меток, но при формировании батча нам надо их дополнить паддингами. Для этого существует Collator – класс, который вызывается при формировании батча. Он принимает набор произвольных объектов из датасета и делает из них тензоры согласно инструкциям. Для задачи классификации последовательности имеется специальный `DataCollatorForTokenClassification`, который добавляет паддинги к токенам и меткам, что нам собственно и нужно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "c7ed66cf",
   "metadata": {
    "cellId": "9d495tvpgsev556hpshxtb"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "599aa031",
   "metadata": {
    "cellId": "mcp8ussx8keczjz9h7wewu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Поля:\n",
      " dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "Индексы токенов:\n",
      " tensor([[  101,  7270, 22961,  1528,  1840,  1106, 21423,  1418,  2495, 12913,\n",
      "           119,   102],\n",
      "        [  101,  1943, 14428,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n",
      "\n",
      "Индексы меток:\n",
      " tensor([[-100,    2,    8,    1,    8,    8,    8,    1,    8,    8,    8, -100],\n",
      "        [-100,    3,    7, -100, -100, -100, -100, -100, -100, -100, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "batch = data_collator([train_dataset[i] for i in range(2)])\n",
    "print('Поля:\\n', batch.keys())\n",
    "print('\\nИндексы токенов:\\n', batch['input_ids'])\n",
    "print('\\nИндексы меток:\\n', batch['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0787c29",
   "metadata": {
    "cellId": "d2hg8sy9wx2geoubt1ecl",
    "execution_id": "abfeec95-e324-4646-b075-5a45404bf898"
   },
   "source": [
    "Теперь мы готовы обернуть всю нашу красоту в `DataLoader`, по которому будем итерироваться при обучении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "090e65a5",
   "metadata": {
    "cellId": "0kjv3jsd8vzliw5ape4hhp"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3622faf4",
   "metadata": {
    "cellId": "x8waa5rlfv8fxcdc4wfvxh",
    "execution_id": "1d50d290-7c92-4f4b-9d06-f6e7ba5d71b5"
   },
   "source": [
    "### Метрика\n",
    "\n",
    "Для оценки качества NER чаще всего используется F1-мера. Разделяют два метода подсчета метрики:\n",
    "1) Token-level: считается правильность предсказания отденьной метки для каждого токена.\n",
    "2) Entity-level: считается правильность предсказания метки для всей сущности целиком независимо от того, сколько слов или токенов в нее входит.\n",
    "\n",
    "Обычно предпочтение отдается второму способу, так как иначе, во-первых, качество зависит от токенизации, а во-вторых, если сущность состоит из нескольких слов и модель выставляет словам разные метки, то становится непонятно, к какому именно классу относить данную сущность. Для практики такой результат настолько же плох, насколько полное неугадывание класса, поэтому странно давать за это баллы.\n",
    "\n",
    "Заметьте, предсказание `[I-PER', 'I-PER]` при верном `[B-PER', 'I-PER]` считается корректным, так как из него можно однозначно восстановить ответ, догадавшись, что не первом месте должно стоять `B-`. В то же время при верном `[B-PER', 'B-PER]` такое предсказание корректным не будет.\n",
    "\n",
    "Для подсчета метрики будем использовать уже готовое [решение](https://huggingface.co/spaces/evaluate-metric/seqeval) из библиотеки `seqeval` (семейство `huggingface`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "604f14d4",
   "metadata": {
    "cellId": "eiz7nb9od1ic6whluejidd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seqeval in /home/jupyter/.local/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.22.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "%pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "32a29ae1",
   "metadata": {
    "cellId": "8xa0ms9mbbb4mgls0nf61n"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from seqeval.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "b1fa4a0e",
   "metadata": {
    "cellId": "2dih7j78yja9pmtczycwwq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5714285714285714, 0.4)"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "# here are 7 entities in total, we guessed correctly 4 of them.\n",
    "\n",
    "predictions = [['O', 'I-PER', 'I-PER', 'O'], ['I-PER', 'I-PER', 'O']]\n",
    "references = [['O', 'B-PER', 'B-PER', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "acc = accuracy_score(predictions, references)\n",
    "f1 = f1_score(references, predictions)\n",
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "e80a8e24",
   "metadata": {
    "cellId": "v9v47t5uyl7lrtnxt32cgi"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def calc_f1(predictions: List[List[int]], labels: List[List[int]]):\n",
    "    \"\"\"\n",
    "    :params:\n",
    "    predictions: list of lists of predicted labels\n",
    "    labels: list of lists of ground truth labels\n",
    "    \"\"\"\n",
    "    text_labels = [[id2tag[l] for l in label if l != -100] for label in labels]\n",
    "    text_predictions = []\n",
    "    for i in range(len(text_labels)):\n",
    "        # +1 because we skip the first ([CLS]) token\n",
    "        sample_text_preds = [id2tag[predictions[i][j + 1]] for j in range(len(text_labels[i]))]\n",
    "        text_predictions.append(sample_text_preds)\n",
    "\n",
    "    return f1_score(text_labels, text_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5831cf0e",
   "metadata": {
    "cellId": "gwfm0fovx6iivj15xxtu2h",
    "execution_id": "c583e10f-a819-4bab-9f6a-0b44f43e7989"
   },
   "source": [
    "### Модель\n",
    "\n",
    "В качестве начальной модели мы будем использовать предобученный BERT, а если быть точнее `bert-base-cased` из библиотеки `huggingface`. Он содержит 107М параметров. В последующих заданиях мы будем реализовывать методы для уменьшения его размеров с минимальной потерей качества.\n",
    "\n",
    "Для классификации последовательностей в `transformers` существует специальная обертка `AutoModelForTokenClassification`. Воспользуемся ею и обернем нашу модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "66fc5b4d",
   "metadata": {
    "cellId": "aahhhqgy8gcajkmc4p954s"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38bb64d405a64729bc060eff5cf8ace5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 107726601\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "model = AutoModelForTokenClassification.from_pretrained('bert-base-cased', id2label=id2tag, label2id=tag2id).to(device)\n",
    "print('Number of parameters:', sum([p.numel() for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "f9e38707",
   "metadata": {
    "cellId": "x90v0htwt78iy1mj8rf53p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 54])\n",
      "torch.Size([32, 54])\n",
      "0.01282051282051282\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "# Inference test\n",
    "for x in train_loader:\n",
    "    x['input_ids'] = x['input_ids'].to(device)\n",
    "    x['attention_mask'] = x['attention_mask'].to(device)\n",
    "    x['labels'] = x['labels'].to(device)\n",
    "    out = model(**x)\n",
    "    print(out.logits.argmax(2).shape)\n",
    "    print(x['labels'].shape)\n",
    "    print(calc_f1(out.logits.argmax(2).tolist(), x['labels'].tolist()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a2d475",
   "metadata": {
    "cellId": "pnrk1atye4intybvjmja",
    "execution_id": "24b7e6ec-578e-48c5-afb5-04dd852c8e1f"
   },
   "source": [
    "## Обучение всякого"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b579fe1",
   "metadata": {
    "cellId": "wo964vvudynrt1400w8p8",
    "execution_id": "558e956a-d61e-4021-bf0a-0f4e7e1a4ce6"
   },
   "source": [
    "**Задание 3 (1 балл)** Все методы уменьшения размерности основываются на том, что у нас есть некоторая обученная модель. Сейчас у нас есть предобученный BERT, но на задачу MLM, а не NER. Дообучите BERT на нашем датасете. Ориентировочно у вас должно получиться значение F1 не меньше 0.93 на валидационной выборке. Само обучение никак не должно занимать больше получаса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "4b8f3e7e",
   "metadata": {
    "cellId": "szdby9t9kspbycsaammhm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "469"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "228c1e8f",
   "metadata": {
    "cellId": "f1b02qf7r2e4n877vobx73"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from train import train\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "scheduler = OneCycleLR(optimizer=optimizer, max_lr=1e-5, epochs=30, steps_per_epoch=len(train_loader), pct_start=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4d6003",
   "metadata": {
    "cellId": "ozbjrvq87mpjv2ytqa8vh"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train(30, model, optimizer, train_loader, valid_loader, device, scheduler=scheduler, log_step=50, metric=calc_f1, run_name=\"bert-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638720dc",
   "metadata": {
    "cellId": "hyzu1ygbslinndailavnr",
    "execution_id": "c07e2cbd-6e31-44f9-a3af-ec341e47b17b"
   },
   "source": [
    "run link: https://api.wandb.ai/links/wbot/d7ilosqz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac72e9a",
   "metadata": {
    "cellId": "d9vec3m3m5fieqpqw3rzsr"
   },
   "source": [
    "### Embedding factorization\n",
    "\n",
    "Можно заметить, что на данный момент матрица эмбеддингов занимает $V \\cdot H = 28996 \\cdot 768 = 22.268.928$ параметров. Это целая пятая часть от всей модели! Давайте попробуем с этим что-то сделать. В вариации [ALBERT](https://arxiv.org/pdf/1909.11942.pdf) предлагается факторизовать матрицу эмбеддингов в произведение двух небольших матриц. Таким образом, параметры эмбеддингов будут содержать $V \\cdot E + E \\cdot H$ элементов, что гораздо меньше, если $H \\gg E$. Авторы выбирают $E = 128$, однако ничего не мешает вам взять значение меньше.\n",
    "\n",
    "__Задание 4 (1 балл)__ Замените слой эмбеддингов на описанную факторизацию и дообучите полученную в предыдущем задании модель. Насколько вам удалось уменьшить число параметров? Если вы все сделали правильно, то F1-мера на валидации не должна опуститься ниже 0.9.\n",
    "\n",
    "Мы настоятельно рекомендуем переиспользовать код для обучения из предыдущего задания и не создавать новую функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "id": "ee0b55a7",
   "metadata": {
    "cellId": "o46yr8sdftmkaoq7lbtzjk"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "embed_weights = model.bert.embeddings.word_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "fef32a33",
   "metadata": {
    "cellId": "dgkaxo79vxnou9mmm7wvig"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "u, s, v = torch.svd(embed_weights, some=True, compute_uv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "427c1657",
   "metadata": {
    "cellId": "wd97g6d9iadrg17f657pwp"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "u = u[:, :128]\n",
    "s = s[:128]\n",
    "v = v[:128, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "5ab7a944",
   "metadata": {
    "cellId": "yrk9f35272h4z6tuy94bc"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "u = u @ torch.diag(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "858a0d8e",
   "metadata": {
    "cellId": "8natu1xchoo8g1jmmqtm3s"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from model import FacEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "d429d585",
   "metadata": {
    "cellId": "7nl9plj52zfvgv105aozo"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "fac_e = FacEmbedding(u.shape[0], v.shape[1], 128, u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "d6e7fb37",
   "metadata": {
    "cellId": "57e4daii2684bsriylhx16"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import copy\n",
    "\n",
    "model_fac = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "28b3498e",
   "metadata": {
    "cellId": "6pj7myp7k87481s536whr"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model_fac.bert.embeddings.word_embeddings = fac_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "733de3af",
   "metadata": {
    "cellId": "pbg1n65211ly2waw9lcftr"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "optimizer_fac = AdamW(model_fac.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "scheduler_fac = OneCycleLR(optimizer=optimizer_fac, max_lr=1e-5, epochs=30, steps_per_epoch=len(train_loader), pct_start=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a24327",
   "metadata": {
    "cellId": "nsrnqmfdzt69qevvyieyw"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train(30, model_fac, optimizer_fac, train_loader, valid_loader, device, scheduler=scheduler_fac, log_step=50, metric=calc_f1, run_name=\"bert-fac-1\", checkpoint_name=\"checkpoint-fac\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee7a7f6",
   "metadata": {
    "cellId": "ktqj1cji5ppqzwj743015n",
    "execution_id": "6b9cfdd5-97c1-4d8a-ac09-a6eb0e2a087f"
   },
   "source": [
    "run link: https://api.wandb.ai/links/wbot/u22ozwg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90bcfbe",
   "metadata": {
    "cellId": "n9o5kk7uztlcvo18qft6x"
   },
   "source": [
    "### Дистилляция знаний\n",
    "\n",
    "Дистилляция знаний – это парадигма обучения, в которой знания модели-учителя дистиллируются в модель-ученика. Учеником может быть произвольная модель меньшего размера, решающая ту же задачу. При дистилляции используются два функционала ошибки:\n",
    "\n",
    "1. Стандартная кросс-энтропия.\n",
    "1. Функция, задающая расстояние между распределениями предсказаний учителя и ученика. Чаще всего используют кросс-энтропию или KL-дивергенцию.\n",
    "\n",
    "При этом для того, чтобы распределение предсказаний учителя не было таким вырожденным используют softmax с температурой больше 1, например, 2 или 5.\n",
    "\n",
    "<img src=\"https://intellabs.github.io/distiller/imgs/knowledge_distillation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d8e73e",
   "metadata": {
    "cellId": "y6fm65g0ipj22dt9b3td8j"
   },
   "source": [
    "__Задание 5 (1 балл)__ Реализуйте метод дистилляции знаний, изображенный на картинке. Для подсчета ошибки между предсказаниями ученика и учителя используйте KL-дивергенцию (`nn.KLDivLoss(reduction=\"batchmean\")`). В качестве учителя используйте дообученный BERT из задания 3. В качестве ученика вы можете взять произвольную необученную модель с размером около 40M параметров. Не забудьте про warmup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "id": "60fcced1",
   "metadata": {
    "cellId": "1y90vi8wgtugfmhorsax1o"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from train import train_distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "id": "22960639",
   "metadata": {
    "cellId": "1vs0hslexp78b09d15qdhl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 782,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "model = AutoModelForTokenClassification.from_pretrained('bert-base-cased', id2label=id2tag, label2id=tag2id).to(device)\n",
    "checkpoint = torch.load(\"checkpoint.pth\")\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "id": "8cbe509f",
   "metadata": {
    "cellId": "4xs3qfpoxvaw89aya69uf"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "conf = copy.deepcopy(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "id": "e3855f47",
   "metadata": {
    "cellId": "unvg0dols6kdprmyoz0a"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from transformers import BertConfig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "id": "631a523d",
   "metadata": {
    "cellId": "blj662vheavp6n6wyry1o"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# from model import Model\n",
    "# model_student = Model()\n",
    "# checkpoint = torch.load(\"weights-2.pt\", map_location=torch.device('cuda'))\n",
    "# model_student.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "id": "e38999d0",
   "metadata": {
    "cellId": "767w5dkny6yfnmodgeswxc"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# checkpoint = torch.load(\"checkpoint-distill-2.pth\")\n",
    "# torch.save(checkpoint['state_dict'], \"weights-2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "id": "26bd5638",
   "metadata": {
    "cellId": "rn73ps7rxxkpu2f6592io"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 89268233\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "from transformers import BertForTokenClassification\n",
    "model_student = BertForTokenClassification(conf)\n",
    "model_student.bert.embeddings.word_embeddings = fac_e\n",
    "model_student.to(device)\n",
    "print('Number of parameters:', sum([p.numel() for p in model_student.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "id": "cbfc9c8a",
   "metadata": {
    "cellId": "cxocyz3eghegbia2o1djg9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 17831945\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "for i in range(1, len(model_student.bert.encoder.layer)):\n",
    "    model_student.bert.encoder.layer[i].attention.self.query = model_student.bert.encoder.layer[0].attention.self.query\n",
    "    model_student.bert.encoder.layer[i].attention.self.key = model_student.bert.encoder.layer[0].attention.self.key\n",
    "    model_student.bert.encoder.layer[i].attention.self.value = model_student.bert.encoder.layer[0].attention.self.value\n",
    "    model_student.bert.encoder.layer[i].intermediate = model_student.bert.encoder.layer[0].intermediate\n",
    "    model_student.bert.encoder.layer[i].output.dense = model_student.bert.encoder.layer[0].output.dense\n",
    "print('Number of parameters:', sum([p.numel() for p in model_student.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "id": "bd80feb3",
   "metadata": {
    "cellId": "eux7pgfqkvlm6p3zegfo87"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# checkpoint = torch.load(\"weights.pt\", map_location=torch.device('cuda'))\n",
    "# model_student.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "id": "653b9fa9",
   "metadata": {
    "cellId": "hbdg4cgr6djbpb3bjc6ska"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "optimizer_d = AdamW(model_student.parameters(), lr=2e-4, weight_decay=1e-2)\n",
    "scheduler_d = OneCycleLR(optimizer=optimizer_d, max_lr=2e-4, epochs=60, steps_per_epoch=len(train_loader), pct_start=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66052d3c",
   "metadata": {
    "cellId": "mw3hjn2candogqg84mlg9b",
    "execution_id": "7a3ca438-9b25-4714-80cb-61a4b83d1128"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_distillation(60, \n",
    "                   model,\n",
    "                   model_student,\n",
    "                   optimizer_d, \n",
    "                   train_loader, \n",
    "                   valid_loader, \n",
    "                   device, \n",
    "                   scheduler=scheduler_d, \n",
    "                   log_step=50, \n",
    "                   metric=calc_f1, \n",
    "                   run_name=\"bert-distill-3\", \n",
    "                   clip=100,\n",
    "                   checkpoint_name=\"checkpoint-distill-3\",\n",
    "                   t=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "ea186c23",
   "metadata": {
    "cellId": "ihphnkpgsaqg88rfwk6x0o"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# from train import save_checkpoint\n",
    "# save_checkpoint(model_student, optimizer_d, 60, scheduler_d, \"checkpoint-distill-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6000f9f",
   "metadata": {
    "cellId": "1mp0qyti5g5nv0yjftg5ih",
    "execution_id": "67dcba0f-0fc3-4c3e-b135-31c051ab889f"
   },
   "source": [
    "Как-то так выходит: https://api.wandb.ai/links/wbot/4r28rpik\n",
    "\n",
    "У экспериментов, по сути, отличается только lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f96d697",
   "metadata": {
    "cellId": "gg2fni3t1llhi64u2aauwi",
    "tags": []
   },
   "source": [
    "## Соревнование (до 10 баллов)\n",
    "\n",
    "Ваша задача – обучить модель с размером __не больше 20М параметров__ для задачи NER. При этом можно пользоваться предобученным `bert-base-cased`, но больше ничем. \n",
    "\n",
    "Соревнование будет проходить аналогично соревнованию из второго домашнего задания. Ваши посылки вы должны будете отправлять тг-боту [@nlp_hw4_bot](t.me/nlp_hw4_bot), а он будет считать значения F1 на публичном и приватном датасетах и записывать результат в [табличку](https://docs.google.com/spreadsheets/d/1rILRI16VxgztwlfqR2kPZ3MxlJkerz6iEr5Kx9yrOLA/edit#gid=0).\n",
    "\n",
    "Для формирования посылки вам нужно будет создать папку на dropbox, положить в нее файл `model.py` с классом модели `Model` и веса `weights.pt`, а затем отправить боту ссылку на эту папку, доступную к чтению. Бот будет импортировать модель и загружать веса:\n",
    "```\n",
    "module = __import__('model', globals(), locals(), ['Model'], 0)\n",
    "model = module.Model()\n",
    "model.load_state_dict(torch.load('weights.pt', map_location=torch.device('cpu')))\n",
    "```\n",
    "\n",
    "При тестировании модель будет получать на вход `input_ids` и `attention_mask`, а на выход должна возвращать трехмерный тензор с вероятностями меток для каждого токена в батче. Класс `Model` должен содержать аргумент `id2label` совпадающий с тем, который задан в конфиге модели `model.config.id2label`. Это нужно для того, чтобы id тэгов мапились в нужные названия тэгов, так как они могут отличаться у разных решений.\n",
    "\n",
    "\n",
    "__Обязательм условием__ участия в соревновании является отчет о проделанной работе в формате pdf, в котором вы должны описать опробованные методы с результатами. За отчет выставляется максимум до __2 баллов__ на усмотрение проверяющего. В случае отсутствия отчета баллы за соревнование __обнуляются__.\n",
    "\n",
    "После дедлайна по домашке будут выложен _приватный_ лидерборд, по которому и будут выставляться баллы за соревнования. За место в лидерборде можно получить до __8__ баллов, но только при условии, если вы получили больше __0.8__ на _публичном_ лидерборде, в противном случае баллы выставляться не будут.\n",
    "$$\n",
    "\\text{число баллов} = 8\\frac{(N - r + 1)}{N},\n",
    "$$\n",
    "где $r$ – место в лидерборде, а $N$ - число участников со значением F1 на _публичном_ лидерборде не меньше __0.8__.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "id": "66bf58a7",
   "metadata": {
    "cellId": "5wszudbw79bsfgaam6nk4j"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from model import FacEmbedding\n",
    "\n",
    "embed_weights = model.bert.embeddings.word_embeddings.weight\n",
    "u, s, v = torch.svd(embed_weights, some=True, compute_uv=True)\n",
    "u = u[:, :64]\n",
    "s = s[:64]\n",
    "v = v[:64, :]\n",
    "u = u @ torch.diag(s)\n",
    "fac_e = FacEmbedding(u.shape[0], v.shape[1], 64, u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "id": "e7a7e844",
   "metadata": {
    "cellId": "esqv7gwfuxctyxfgscu058"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "conf = copy.deepcopy(model.config)\n",
    "conf.num_hidden_layers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "id": "6f99f546",
   "metadata": {
    "cellId": "n5z0sz05ncb13coox2ppz9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 59011849\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "from transformers import BertForTokenClassification\n",
    "model_student = BertForTokenClassification(conf)\n",
    "model_student.bert.embeddings.word_embeddings = fac_e\n",
    "model_student.to(device)\n",
    "print('Number of parameters:', sum([p.numel() for p in model_student.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "id": "5bf3c6e9",
   "metadata": {
    "cellId": "fmhqvu1cb0lloeb4ri6plp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 20046601\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "for i in range(1, 4):\n",
    "    model_student.bert.encoder.layer[i].attention.self.query = model_student.bert.encoder.layer[0].attention.self.query\n",
    "    model_student.bert.encoder.layer[i].attention.self.key = model_student.bert.encoder.layer[0].attention.self.key\n",
    "    model_student.bert.encoder.layer[i].attention.self.value = model_student.bert.encoder.layer[0].attention.self.value\n",
    "    model_student.bert.encoder.layer[i].intermediate = model_student.bert.encoder.layer[0].intermediate\n",
    "    model_student.bert.encoder.layer[i].output.dense = model_student.bert.encoder.layer[0].output.dense\n",
    "    \n",
    "for i in range(5, 8):\n",
    "    model_student.bert.encoder.layer[i].attention.self.query = model_student.bert.encoder.layer[4].attention.self.query\n",
    "    model_student.bert.encoder.layer[i].attention.self.key = model_student.bert.encoder.layer[4].attention.self.key\n",
    "    model_student.bert.encoder.layer[i].attention.self.value = model_student.bert.encoder.layer[4].attention.self.value\n",
    "    model_student.bert.encoder.layer[i].intermediate = model_student.bert.encoder.layer[4].intermediate\n",
    "    model_student.bert.encoder.layer[i].output.dense = model_student.bert.encoder.layer[4].output.dense\n",
    "print('Number of parameters:', sum([p.numel() for p in model_student.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "id": "b91bf78f",
   "metadata": {
    "cellId": "grwgbjjwvirqhtzsjf9z8j"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-20 08:49:40.356649: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "from train import train\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "optimizer = AdamW(model_student.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "scheduler = OneCycleLR(optimizer=optimizer, max_lr=1e-4, epochs=60, steps_per_epoch=len(train_loader), pct_start=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bf6e06",
   "metadata": {
    "cellId": "mzsqm7irqcjbashxkizg"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train(60, model_student, optimizer, train_loader, valid_loader, device, scheduler=scheduler, log_step=50, metric=calc_f1, run_name=\"bert-small\", checkpoint_name=\"checkpoint-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "id": "57f469b3",
   "metadata": {
    "cellId": "jh5e7ptiqblb320jcipap"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "checkpoint = torch.load(\"checkpoint-small.pth\")\n",
    "torch.save(checkpoint['state_dict'], \"weights-3.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e5651",
   "metadata": {
    "cellId": "sa9mzcvoc0rxlps5bx8vzb"
   },
   "source": [
    "### Что стоит попробовать?\n",
    "\n",
    "* В статье [ALBERT](https://arxiv.org/abs/1909.11942) помимо факторизации эмбеддингов предлагается использовать одни и те же параметры для нескольких слоев. Такой подход позволяет серьезно уменьшить число параметров.\n",
    "\n",
    "* В задании 5 мы инициализировали ученика случайно, однако можно сделать лучше. При дистилляции знаний для downstream задачи из предобученного в unsupervised формате учителя (задача MLM) часто помогает сперва дистиллировать модель для задачи предобучения, а затем ее уже дообучать на downstream задачу с соответствующим учителем. Другими словами, лучше сначала дистиллировать предобученный BERT в ученика на MLM задаче, а затем использовать этого ученика в качестве начальной инициализации для второй дистилляции.\n",
    "\n",
    "* При дистилляции мы выравниваем только предсказания моделей, однако можно выравнивать еще и скрытые слои. Например, приближать матрицы внимания и выходы каждого скрытого слоя. Подробнее об этом можно почитать [тут](https://www.researchgate.net/publication/375758425_Knowledge_Distillation_Scheme_for_Named_Entity_Recognition_Model_Based_on_BERT).\n",
    "\n",
    "* В данный момент мы используем все головы внимания, но ряд исследований показывает, что большинство из них можно выбросить без потери качества. В этой [статье](https://arxiv.org/pdf/1905.09418.pdf) предлагается следующий подход. Добавим гейт $g_i \\in \\{0, 1\\}$ для каждой головы внимания.\\\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_I^V)\n",
    "$$\n",
    "$$\n",
    "\\text{MultiHeadAttention}(Q, K, V) = \\text{Concat}(g_i \\cdot \\text{head}_i) W^O\n",
    "$$\n",
    "---\n",
    "__Теоретический блок для тех, кому интересно__   \n",
    "Будем настраивать значения гейтов в процессе обучения. Мы хотим, чтобы как можно большая часть гейтов стала нулями, поэтому добавим в функционал модели $L_0$ регуляризацию на $g_i$. Проблема в том, что $L_0$ – недифференцируемая функция, поэтому нам надо релаксировать ее.\n",
    "Будем считать, что каждый гейт задается распределением Бернулли, $g_i \\sim \\text{Bernoulli}(\\alpha_i)$, где $\\alpha_i$ – настраиваемый параметр. Осталось понять, как его настраивать. Если мы будем напрямую семплировать из распределения Бернулли, то мы потеряем связь между $\\alpha_i$ и семплом, поэтому мы не сможем прокинуть градиенты (такая же проблема возникает в VAE, там используют reparametrization trick). Существует хороший способ семплирования дискретных случайных величин с сохранением связи – __Gumbel-Max trick__. Пусть вероятность каждого значения случайной величины пропорциональна $\\beta_k \\in (0, \\infty)$ и \\\n",
    "$$\n",
    "x = \\text{argmax}_k \\{\\log \\beta_k - \\log(-\\log(\\text{Uniform}(0, 1))) \\},\n",
    "$$\n",
    "Тогда $P(x = k) = \\frac{\\beta_k}{\\sum_k \\beta_k}$. Для того, чтобы избавиться от недифференцируемого аргмакса можно релаксировать его, заменив на softmax с температурой меньше 1, так мы получим [Concrete distribution](https://arxiv.org/pdf/1611.00712.pdf). Теперь мы сможем в процессе обучения семплировать значения гейтов и обновлять $\\alpha_i$ градиентным спуском. Почти победа, осталось понять, при каких значениях $\\alpha_i$ после обучения мы будем считать, что гейт закрыт и голову можно выбросить. В [статье](https://arxiv.org/pdf/1712.01312.pdf) про Hard Concrete распределение, предложившей $L_0$ регуляризацию, предлагается немного растянуть распределение вероятности открытия гейта с $[0, 1]$ до $[\\gamma, \\zeta]$ (например, $[-0.1, 1.1]$), а затем обрезать его обратно до $[0, 1]$. Таким образом, все значения, которые были меньше 0, превратятся в 0. Теперь мы будем считать гейт закрытым, если мы получили на выходе 0.\n",
    "$$g_i = \\text{clip}\\big(\\text{Sigmoid}(\\log \\alpha_i)(\\zeta - \\gamma) + \\gamma, 0, 1\\big)$$\n",
    "---\n",
    "Получаем следующий алгоритм подбора значений для гейтов.\n",
    "1. Заводим параметр $\\log \\alpha_i$ для каждой головы каждого слоя.\n",
    "2. Добавляем к функционалу ошибки слагаемое регуляризации с небольшим коэффициентом $\\lambda$ (например, $0.02$)\n",
    "$$\n",
    "\\mathcal{L}_C = \\sum_{i=1}^h (1 - P(g_i = 1 | \\alpha_i)) = \\sum_{i=1}^h \\text{Sigmoid}\\Big(\\log \\alpha_i - \\tau \\log \\frac{-\\gamma}{\\zeta}\\Big),\n",
    "$$\n",
    "где $\\gamma$ < 0 и $\\zeta$ > 1, $\\tau$ – гиперпараметры (можно взять $-0.1$, $1.1$ и $0.33$ соответственно)\n",
    "3. При каждом вызове модели значения гейтов семплируются из Hard Concrete распределения.\n",
    "\\begin{align}\n",
    "u &= \\text{Uniform}(0, 1) \\\\\n",
    "z &= \\text{Sigmoid}\\big((\\log u - \\log(1 - u) + \\log \\alpha_i) \\,/\\, \\tau\\big) \\\\\n",
    "g_i &= \\text{clip}\\big( z (\\zeta - \\gamma) + \\gamma, 0, 1 \\big)\n",
    "\\end{align}\n",
    "4. После обучения в идеале выбрасываются головы, для которых $\\text{clip}\\big(\\text{Sigmoid}(\\log \\alpha_i)(\\zeta - \\gamma) + \\gamma, 0, 1\\big)$ равняется 0. Если таких нет или очень мало, то можно выбросить те, которые близки к нулю, а затем дообучить модель без этих голов.\n",
    "\n",
    "\n",
    "[Тут](https://arxiv.org/pdf/2110.03252.pdf) можно почитать про дополнительные хаки для этого метода.\\\n",
    "\\\n",
    "P. S. Заводится тяжело, но заводится. Если гейты не начинают зануляться, то, возможно, вы недостаточно долго учите.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945038f3",
   "metadata": {
    "cellId": "1lkgdthdpheiecu9lamli"
   },
   "source": [
    "Помимо всего, что написано выше, на просторах интернета можно найти кучу других способов уменьшения модели, не стестяйтесь их искать. При проведении экспериментов старайтесь делать одно изменение за раз и не бросайтесь реализовывать сложный метод, если у вас нет достаточных оснований полагать, что он даст значительный буст.\n",
    "\n",
    "Удачи!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a0e233",
   "metadata": {
    "cellId": "3eh7jkcy7qff68ung7bfs9"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "ab05570b-4a02-4c97-8f54-e3995c184002",
  "notebookPath": "hw4.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
